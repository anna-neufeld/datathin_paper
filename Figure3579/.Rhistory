# Data Thinning (0.5)
eps <- 0.5
sp <- binomsplit(X, epsilon=eps, pop=N)
Ndt <- N*eps
p.hat <- (sp$Xtr+0.001)/(Ndt+0.002)
logitp.hat <- qlogis(p.hat)
dt.svd <- svd(logitp.hat)
for (j in 1:maxPC) {
# Compute reconstruction errors and save results
results_binom[counter,] <- c("Data Thinning (0.5)", i, "MSE", j, recon.bin(sp$Xte, dt.svd, Ndt*(1-eps)/eps, j))
counter <- counter + 1
results_binom[counter,] <- c("Data Thinning (0.5)", i, "NLL", j, recon.nll(sp$Xte, dt.svd, Ndt*(1-eps)/eps, j))
counter <- counter + 1
}
# Data Thinning (0.8)
eps <- 0.8
sp <- binomsplit(X, epsilon=eps, pop=N)
Ndt <- N*eps
p.hat <- (sp$Xtr+0.001)/(Ndt+0.002)
logitp.hat <- qlogis(p.hat)
dt.svd <- svd(logitp.hat)
for (j in 1:maxPC) {
# Compute reconstruction errors and save results
results_binom[counter,] <- c("Data Thinning (0.8)", i, "MSE", j, recon.bin(sp$Xte, dt.svd, Ndt*(1-eps)/eps, j))
counter <- counter + 1
results_binom[counter,] <- c("Data Thinning (0.8)", i, "NLL", j, recon.nll(sp$Xte, dt.svd, Ndt*(1-eps)/eps, j))
counter <- counter + 1
}
# Multifold Data Thinning
spcv <- multibinom(X, maxf, arg=N) %>%
purrr::map(~mutate(as.data.frame(.x), idx = row_number())) %>%
bind_rows(.id="fold") %>%
mutate(fold = as.numeric(fold))
Nmft <- N*(maxf-1)/maxf
mftreal <- matrix(0, nrow=maxf, ncol=maxPC)
mftnll <- matrix(0, nrow=maxf, ncol=maxPC)
for (f in 1:maxf) {
## Reaggregate folds into train/test sets
CVtr <- spcv %>%
filter(fold != f) %>%
group_by(idx) %>%
summarise(across(-fold, sum)) %>%
select(-idx) %>%
as.matrix
CVte <- spcv %>%
filter(fold == f) %>%
select(-fold, -idx) %>%
as.matrix
p.hat <- (CVtr+0.001)/(Nmft+0.002)
logitp.hat <- qlogis(p.hat)
mft.svd <- svd(logitp.hat)
for (j in 1:maxPC) {
# Compute reconstruction errors and save results
mftreal[f,j] <- recon.bin(CVte, mft.svd, Nmft*1/(maxf-1), j)
mftnll[f,j] <- recon.nll(CVte, mft.svd, Nmft*1/(maxf-1), j)
}
}
for (j in 1:maxPC) {
# Compute reconstruction errors and save results
results_binom[counter,] <- c("Multifold Thinning", i, "MSE", j, mean(mftreal[,j]))
counter <- counter + 1
results_binom[counter,] <- c("Multifold Thinning", i, "NLL", j, mean(mftnll[,j]))
counter <- counter + 1
}
}
# Set the small gamma clustering simulation parameters
n <- 100
d <- 2
ncluster <- 4
true_clusters <- truth <- rep(c(1:ncluster), n)
## Gamma parameters for the "small d, small K" setting
pars <- data.frame(lambda = c(20,20,20,20,20,20,20,20),
theta = c(0.5,5,5,0.5,10,10,0.5,0.5))
lambda <- matrix(rep(pars$lambda,n), nrow=ncluster*n, ncol=d, byrow=TRUE)
theta <- matrix(rep(pars$theta,n), nrow=ncluster*n, ncol=d, byrow=TRUE)
maxk <- 10
results_gammasmall <- matrix(0, nrow=2*nreps*4*maxPC, ncol=5)
colnames(results_gammasmall) <- c("method", "sim", "measure", "k", "value")
## Run the simulations
counter <- 1
trial=1
#Generate Gamma data with the specified number of dimensions then split
Z <- matrix(0, nrow=n*ncluster, ncol=d)
Z[] <- rgamma(n*d*ncluster, shape=lambda, rate=theta)
sp <- gammasplit(Z, epsilon=0.5, shape=lambda)
sp2 <- gammasplit(Z, epsilon=0.8, shape=lambda)
spcv <- multigamma(Z, maxf, arg=lambda) %>%
purrr::map(~mutate(as.data.frame(.x), idx = row_number())) %>%
bind_rows(.id="fold") %>%
mutate(fold = as.numeric(fold))
for (k in 1:maxk) {
#First, consider the traditional within sum of squares approach
sskmeans <- kmeans(Z, centers=k, nstart=10)
results_gammasmall[counter,] <- c("Naive", trial, "MSE", k, sskmeans$tot.withinss/(n*ncluster))
counter <- counter + 1
results_gammasmall[counter,] <- c("Naive", trial, "NLL", k, gammaNLL3(sskmeans, Z, Z, "naive", 0.5))
counter <- counter + 1
#Then, consider the count splitting approach with eps = 0.5
cskmeans <- kmeans(sp$Xtr, centers=k, nstart=10)
results_gammasmall[counter,] <- c("Data Thinning (0.5)", trial, "MSE", k, GCSMSE(cskmeans, sp$Xte, 0.5))
counter <- counter + 1
results_gammasmall[counter,] <- c("Data Thinning (0.5)", trial, "NLL", k, gammaNLL3(cskmeans, sp$Xtr, sp$Xte, "GCS", 0.5))
counter <- counter + 1
#Then, consider the count splitting approach with eps = 0.8
cskmeans <- kmeans(sp2$Xtr, centers=k, nstart=10)
results_gammasmall[counter,] <- c("Data Thinning (0.8)", trial, "MSE", k, GCSMSE(cskmeans, sp2$Xte, 0.8))
counter <- counter + 1
results_gammasmall[counter,] <- c("Data Thinning (0.8)", trial, "NLL", k, gammaNLL3(cskmeans, sp2$Xtr, sp2$Xte, "GCS", 0.8))
counter <- counter + 1
#Then, consider count splitting cross-validation strategy
cvmse <- rep(0, maxf)
cvnll <- rep(0, maxf)
for (f in 1:maxf) {
CVtr <- spcv %>% filter(fold != f) %>% group_by(idx) %>% summarise(across(-fold, sum)) %>%
select(-idx) %>% as.matrix
CVte <- spcv %>% filter(fold == f) %>% select(-fold, -idx) %>% as.matrix
cvkmeans <- kmeans(CVtr, centers=k, nstart=10)
cvmse[f] <- GCSMSE(cvkmeans, CVte, (maxf-1)/maxf)
cvnll[f] <- gammaNLL3(cvkmeans, CVtr, CVte, "GCS", (maxf-1)/maxf)
}
results_gammasmall[counter,] <- c("Multifold Thinning", trial, "MSE", k, mean(cvmse))
counter <- counter + 1
results_gammasmall[counter,] <- c("Multifold Thinning", trial, "NLL", k, mean(cvnll))
counter <- counter + 1
}
k=1
#First, consider the traditional within sum of squares approach
sskmeans <- kmeans(Z, centers=k, nstart=10)
results_gammasmall[counter,] <- c("Naive", trial, "MSE", k, sskmeans$tot.withinss/(n*ncluster))
counter <- counter + 1
results_gammasmall[counter,] <- c("Naive", trial, "NLL", k, gammaNLL3(sskmeans, Z, Z, "naive", 0.5))
counter <- counter + 1
#Then, consider the count splitting approach with eps = 0.5
cskmeans <- kmeans(sp$Xtr, centers=k, nstart=10)
results_gammasmall[counter,] <- c("Data Thinning (0.5)", trial, "MSE", k, GCSMSE(cskmeans, sp$Xte, 0.5))
counter <- counter + 1
results_gammasmall[counter,] <- c("Data Thinning (0.5)", trial, "NLL", k, gammaNLL3(cskmeans, sp$Xtr, sp$Xte, "GCS", 0.5))
counter <- counter + 1
#Then, consider the count splitting approach with eps = 0.8
cskmeans <- kmeans(sp2$Xtr, centers=k, nstart=10)
results_gammasmall[counter,] <- c("Data Thinning (0.8)", trial, "MSE", k, GCSMSE(cskmeans, sp2$Xte, 0.8))
counter <- counter + 1
results_gammasmall[counter,] <- c("Data Thinning (0.8)", trial, "NLL", k, gammaNLL3(cskmeans, sp2$Xtr, sp2$Xte, "GCS", 0.8))
counter <- counter + 1
#Then, consider count splitting cross-validation strategy
cvmse <- rep(0, maxf)
cvnll <- rep(0, maxf)
f=1
CVtr <- spcv %>% filter(fold != f) %>% group_by(idx) %>% summarise(across(-fold, sum)) %>%
select(-idx) %>% as.matrix
CVte <- spcv %>% filter(fold == f) %>% select(-fold, -idx) %>% as.matrix
cvkmeans <- kmeans(CVtr, centers=k, nstart=10)
cvmse[f] <- GCSMSE(cvkmeans, CVte, (maxf-1)/maxf)
cvnll[f] <- gammaNLL3(cvkmeans, CVtr, CVte, "GCS", (maxf-1)/maxf)
gammaNLL3
km <- cvkmeans
traindat <- CVtr
testdat <- CVte
method="GCS"
epsilon = 0.8
ests <- bind_cols(
data.frame(cluster=km$cluster),
as.data.frame(traindat)
) %>%
pivot_longer(-cluster, names_to="dim") %>%
group_by(cluster, dim) %>%
summarise(N=n(),
scale=ifelse(N == 1, 1/value, mean(value*log(value))-mean(value)*mean(log(value))),
shape=ifelse(N == 1, 1, mean(value)/scale), verbose=F) %>%
mutate(scale = ifelse(N > 2, (N/(N-1))*scale, scale),
shape = ifelse(N > 2, shape - (1/N)*(3*shape - (2/3)*(shape/(1+shape)) - (4/5)*(shape/((1+shape)^2))), shape)) %>%
mutate(rate = 1/scale) %>%
select(-N, -scale)
ests <- bind_cols(
data.frame(cluster=km$cluster),
as.data.frame(traindat)
) %>%
pivot_longer(-cluster, names_to="dim") %>%
group_by(cluster, dim) %>%
summarise(N=n(),
scale=ifelse(N == 1, 1/value, mean(value*log(value))-mean(value)*mean(log(value))),
shape=ifelse(N == 1, 1, mean(value)/scale), silent=T) %>%
mutate(scale = ifelse(N > 2, (N/(N-1))*scale, scale),
shape = ifelse(N > 2, shape - (1/N)*(3*shape - (2/3)*(shape/(1+shape)) - (4/5)*(shape/((1+shape)^2))), shape)) %>%
mutate(rate = 1/scale) %>%
select(-N, -scale)
?suppressMessages
## Compute the Gamma clustering NLL loss
gammaNLL3 <- function(km, traindat, testdat, method, epsilon) {
suppressMessages(
{
ests <- bind_cols(
data.frame(cluster=km$cluster),
as.data.frame(traindat)
) %>%
pivot_longer(-cluster, names_to="dim") %>%
group_by(cluster, dim) %>%
summarise(N=n(),
scale=ifelse(N == 1, 1/value, mean(value*log(value))-mean(value)*mean(log(value))),
shape=ifelse(N == 1, 1, mean(value)/scale)) %>%
mutate(scale = ifelse(N > 2, (N/(N-1))*scale, scale),
shape = ifelse(N > 2, shape - (1/N)*(3*shape - (2/3)*(shape/(1+shape)) - (4/5)*(shape/((1+shape)^2))), shape)) %>%
mutate(rate = 1/scale) %>%
select(-N, -scale)
bind_cols(
data.frame(cluster=km$cluster),
as.data.frame(testdat)
) %>%
pivot_longer(-cluster, names_to="dim") %>%
left_join(ests, by=c("cluster", "dim")) %>%
mutate(nll = -dgamma(value, shape=shape*(1-epsilon)/epsilon, rate=rate, log=TRUE)) %>%
filter(!is.infinite(nll)) %>%
summarise(nll = sum(nll)) %>%
pull
}
)
}
nTrials <- 10000
n <- 100
X <- matrix(rnorm(n*nTrials, mean=5, sd=sqrt(7)), nrow=nTrials)
X
variance.full.esitimates <- apply(X, 1, function(u) mean((u-5)^2))
hist(variance.full.esitimates)
var.guesses <- seq(3,12,by=0.1, freq=F)
var.guesses <- seq(3,12,by=0.1, prob=T)
hist(variance.full.esitimates, prob=T)
variance.full.esitimates <- apply(X, 1, function(u) mean((u-5)^2))
hist(variance.full.esitimates, prob=T)
var.guesses <- seq(3,12,by=0.1)
guess.densities <- dnorm(var.guesses, mean=5, sd=sqrt(100/(2*7^2)))
points(var.guesses, guess.densities)
guess.densities <- dnorm(var.guesses, mean=7, sd=sqrt(100/(2*7^2)))
points(var.guesses, guess.densities)
hist(variance.full.esitimates, prob=T, breaks=100)
var.guesses <- seq(3,12,by=0.1)
guess.densities <- dnorm(var.guesses, mean=7, sd=sqrt(100/(2*7^2)))
points(var.guesses, guess.densities)
points(var.guesses, guess.densities, col="red")
sqrt(100/(2*7^2)
)
guess.densities <- dnorm(var.guesses, mean=7, sd=sqrt(100/(2*7)))
points(var.guesses, guess.densities, col="red")
variance.full.esitimates <- apply(X, 1, function(u) mean((u-5)^2))
hist(variance.full.esitimates, prob=T, breaks=100)
var.guesses <- seq(3,12,by=0.1)
guess.densities <- dnorm(var.guesses, mean=7, sd=sqrt(100/(2*7^2)))
points(var.guesses, guess.densities, col="red")
mean(var.guesses)
var(var.guesses)
sqrt(100/(2*7^2)
sqrt(100/(2*7^2))
mean(var.guesses)
var(var.guesses)
sqrt(100/(2*7^2))
mean(var.guesses)
var(var.guesses)
100/(2*7^2)
(2*7^2)/100
mean(var.guesses)
var(var.guesses)
(2*7^2)/100
mean(var.full.estimates)
var(var.full.estimates)
variance.full.esitimates <- apply(X, 1, function(u) mean((u-mean(u))^2))
hist(variance.full.esitimates, prob=T, breaks=100)
var.guesses <- seq(3,12,by=0.1)
guess.densities <- dnorm(var.guesses, mean=7, sd=sqrt(100/(2*7^2)))
points(var.guesses, guess.densities, col="red")
mean(var.full.estimates)
var(var.full.estimates)
mean(variance.full.esitimates)
var(variance.full.esitimates)
(2*7^2)/100
nTrials <- 100000
n <- 100
X <- matrix(rnorm(n*nTrials, mean=5, sd=sqrt(7)), nrow=nTrials)
variance.full.esitimates <- apply(X, 1, function(u) mean((u-mean(u))^2))
hist(variance.full.esitimates, prob=T, breaks=100)
var.guesses <- seq(3,12,by=0.1)
guess.densities <- dnorm(var.guesses, mean=7, sd=sqrt(100/(2*7^2)))
points(var.guesses, guess.densities, col="red")
mean(variance.full.esitimates)
var(variance.full.esitimates)
(2*7^2)/100
nTrials <- 100000
n <- 100
sig.sq <- 7
mu <- 5
X <- matrix(rnorm(n*nTrials, mean=5, sd=sqrt(sig.sq)), nrow=nTrials)
variance.full.esitimates <- apply(X, 1, function(u) mean((u-mean(u))^2))
hist(variance.full.esitimates, prob=T, breaks=100)
var.guesses <- seq(3,12,by=0.1)
guess.densities <- dnorm(var.guesses, mean=7, sd=sqrt(2*sig.sq^2/n))
points(var.guesses, guess.densities, col="red")
mean(variance.full.esitimates)
var(variance.full.esitimates)
(2*sig.sq^2)/n
nTrials <- 100000
n <- 63
sig.sq <- 7
mu <- 5
X <- matrix(rnorm(n*nTrials, mean=5, sd=sqrt(sig.sq)), nrow=nTrials)
variance.full.esitimates <- apply(X, 1, function(u) mean((u-mean(u))^2))
hist(variance.full.esitimates, prob=T, breaks=100)
var.guesses <- seq(3,12,by=0.1)
guess.densities <- dnorm(var.guesses, mean=7, sd=sqrt(2*sig.sq^2/n))
points(var.guesses, guess.densities, col="red")
mean(variance.full.esitimates)
var(variance.full.esitimates)
(2*sig.sq^2)/n
nTrials <- 100000
n <- 63
sig.sq <- 7
mu <- 5
X <- matrix(rnorm(n*nTrials, mean=5, sd=sqrt(sig.sq)), nrow=nTrials)
variance.full.esitimates <- apply(X, 1, function(u) mean((u-mean(u))^2))
hist(variance.full.esitimates, prob=T, breaks=100)
var.guesses <- seq(3,12,by=0.1)
guess.densities <- dnorm(var.guesses, mean=sig.sq, sd=sqrt(2*sig.sq^2/n))
points(var.guesses, guess.densities, col="red")
mean(variance.full.esitimates)
var(variance.full.esitimates)
(2*sig.sq^2)/n
nTrials <- 100000
n <- 243
sig.sq <- 7
mu <- 5
X <- matrix(rnorm(n*nTrials, mean=5, sd=sqrt(sig.sq)), nrow=nTrials)
variance.full.esitimates <- apply(X, 1, function(u) mean((u-mean(u))^2))
hist(variance.full.esitimates, prob=T, breaks=100)
var.guesses <- seq(3,12,by=0.1)
guess.densities <- dnorm(var.guesses, mean=sig.sq, sd=sqrt(2*sig.sq^2/n))
points(var.guesses, guess.densities, col="red")
mean(variance.full.esitimates)
var(variance.full.esitimates)
(2*sig.sq^2)/n
res <- datathin(X, args=sig.sq, family=Gaussian)
res <- dataathin::datathin(X, args=sig.sq, family="Gaussian")
res <- datathin::datathin(X, args=sig.sq, family="Gaussian")
res <- datathin::datathin(X, arg=sig.sq, family="Gaussian")
?datathin::datathin
sig.sq
res <- datathin::datathin(X, arg=sig.sq, family="gaussian")
nTrials <- 100000
n <- 243
sig.sq <- 7
mu <- 5
X <- matrix(rnorm(n*nTrials, mean=5, sd=sqrt(sig.sq)), nrow=nTrials)
variance.full.esitimates <- apply(X, 1, function(u) mean((u-mean(u))^2))
hist(variance.full.esitimates, prob=T, breaks=100)
var.guesses <- seq(3,12,by=0.1)
guess.densities <- dnorm(var.guesses, mean=sig.sq, sd=sqrt(2*sig.sq^2/n))
points(var.guesses, guess.densities, col="red")
mean(variance.full.esitimates)
var(variance.full.esitimates)
(2*sig.sq^2)/n
set.seed(1)
epsilon = 0.3
X1 <- apply(X, 1, function(u) rnorm(length(u), epsilon*u, sqrt(epsilon*(1-epsilon)*sig.sq)))
X2 <- X-X1
variance.1.esitimates <- apply(X1, 1, function(u) mean((u-mean(u))^2))
X1 <- t(apply(X, 1, function(u) rnorm(length(u), epsilon*u, sqrt(epsilon*(1-epsilon)*sig.sq))))
X2 <- X-X1
variance.1.esitimates <- apply(X1, 1, function(u) mean((u-mean(u))^2))
variance.2.esitimates <- apply(X2, 1, function(u) mean((u-mean(u))^2))
mean(variance.1.esitimates)
mean(variance.2.esitimates)
var(variance.full.esitimates)
var(variance.1.esitimates)
var(variance.2.esitimates)
1/var(variance.1.esitimates)+1/var(variance.2.esitimates)
var.full.estimates
variance.full.estimates
1/var(variance.1.esitimates)+1/var(variance.2.esitimates)
variance.full.esitimates
1/var(variance.full.esitimates)
nTrials <- 100000
1/var(variance.1.esitimates)+1/var(variance.2.esitimates)
1/var(variance.full.esitimates)
cor(as.numeric(X1, as.numeric(X2)))
cor(as.numeric(X1), as.numeric(X2))
mean(as.numeric(X1))
mean(as.numeric(X2))
0.3*5
dim(X1)
dim(X2)
variance.1.esitimates <- apply(X1, 1, function(u) mean((u-mean(u))^2)/epsilon)
variance.2.esitimates <- apply(X2, 1, function(u) mean((u-mean(u))^2)/(1-epsilon))
mean(variance.1.esitimates)
mean(variance.2.esitimates)
var(variance.full.esitimates)
var(variance.1.esitimates)
var(variance.2.esitimates)
(2*sig.sq^2)/n
mean.1.esitimates <- apply(X1, 1, function(u) mean(u/epsilon))
mean.2.esitimates <- apply(X2, 1, function(u) mean(u/(1-epsilon)))
var(mean.1.esitimates)
var(mean.2.esitimates)
var(mean.1.esitimates)/epsilon
var(mean.2.esitimates)/(1-epsilon)
var(mean.1.esitimates)*epsilon
var(mean.2.esitimates)*(1-epsilon)
source("~/datathin_paper/Figure3579/simFunctions.R")
library("tidyr")
library("dplyr")
library("ggplot2")
library("patchwork")
source("simFunctions.R")
# Set the number of simulations
nreps <- 2000
# Set the simulation parameters
maxPC <- 20
maxf <- 5
n <- 250
d <- 100
# Generate latent structure in the logit space
nPC <- 10
sval <- 14:5
results_binom <- matrix(0, nrow=2*nreps*4*maxPC, ncol=5)
colnames(results_binom ) <- c("method", "sim", "measure", "k", "value")
## Run the simulations
counter <- 1
for (i in 1:nreps) {
# Set the binomial parameters
Upop <- pracma::randortho(n)[,1:nPC]
Dpop <- diag(sval)
Vpop <- pracma::randortho(d)[,1:nPC]
logitp <- Upop%*%Dpop%*%t(Vpop)
N <- 100
p <- plogis(logitp)
# Generate binomial data
X <- matrix(0, nrow=n, ncol=d)
X[] <- rbinom(n*d, N, p)
# Naive method that reuses data
p.hat <- (X+0.001)/(N+0.002)
logitp.hat <- qlogis(p.hat)
naive.svd <- svd(logitp.hat)
for (j in 1:maxPC) {
# Compute reconstruction errors and save results
results_binom[counter,] <- c("Naive", i, "MSE", j, recon.bin(X, naive.svd, N, j))
counter <- counter + 1
results_binom[counter,] <- c("Naive", i, "NLL", j, recon.nll(X, naive.svd, N, j))
counter <- counter + 1
}
# Data Thinning (0.5)
eps <- 0.5
sp <- binomsplit(X, epsilon=eps, pop=N)
Ndt <- N*eps
p.hat <- (sp$Xtr+0.001)/(Ndt+0.002)
logitp.hat <- qlogis(p.hat)
dt.svd <- svd(logitp.hat)
for (j in 1:maxPC) {
# Compute reconstruction errors and save results
results_binom[counter,] <- c("Data Thinning (0.5)", i, "MSE", j, recon.bin(sp$Xte, dt.svd, Ndt*(1-eps)/eps, j))
counter <- counter + 1
results_binom[counter,] <- c("Data Thinning (0.5)", i, "NLL", j, recon.nll(sp$Xte, dt.svd, Ndt*(1-eps)/eps, j))
counter <- counter + 1
}
# Data Thinning (0.8)
eps <- 0.8
sp <- binomsplit(X, epsilon=eps, pop=N)
Ndt <- N*eps
p.hat <- (sp$Xtr+0.001)/(Ndt+0.002)
logitp.hat <- qlogis(p.hat)
dt.svd <- svd(logitp.hat)
for (j in 1:maxPC) {
# Compute reconstruction errors and save results
results_binom[counter,] <- c("Data Thinning (0.8)", i, "MSE", j, recon.bin(sp$Xte, dt.svd, Ndt*(1-eps)/eps, j))
counter <- counter + 1
results_binom[counter,] <- c("Data Thinning (0.8)", i, "NLL", j, recon.nll(sp$Xte, dt.svd, Ndt*(1-eps)/eps, j))
counter <- counter + 1
}
# Multifold Data Thinning
spcv <- multibinom(X, maxf, arg=N) %>%
purrr::map(~mutate(as.data.frame(.x), idx = row_number())) %>%
bind_rows(.id="fold") %>%
mutate(fold = as.numeric(fold))
Nmft <- N*(maxf-1)/maxf
mftreal <- matrix(0, nrow=maxf, ncol=maxPC)
mftnll <- matrix(0, nrow=maxf, ncol=maxPC)
for (f in 1:maxf) {
## Reaggregate folds into train/test sets
CVtr <- spcv %>%
filter(fold != f) %>%
group_by(idx) %>%
summarise(across(-fold, sum)) %>%
select(-idx) %>%
as.matrix
CVte <- spcv %>%
filter(fold == f) %>%
select(-fold, -idx) %>%
as.matrix
p.hat <- (CVtr+0.001)/(Nmft+0.002)
logitp.hat <- qlogis(p.hat)
mft.svd <- svd(logitp.hat)
for (j in 1:maxPC) {
# Compute reconstruction errors and save results
mftreal[f,j] <- recon.bin(CVte, mft.svd, Nmft*1/(maxf-1), j)
mftnll[f,j] <- recon.nll(CVte, mft.svd, Nmft*1/(maxf-1), j)
}
}
for (j in 1:maxPC) {
# Compute reconstruction errors and save results
results_binom[counter,] <- c("Multifold Thinning", i, "MSE", j, mean(mftreal[,j]))
counter <- counter + 1
results_binom[counter,] <- c("Multifold Thinning", i, "NLL", j, mean(mftnll[,j]))
counter <- counter + 1
}
}
setwd("~/datathin_paper/Figure3579")
nreps <- 2000
source("~/datathin_paper/Figure3579/runSims.R")
